[
  {
    "path": "posts/2021-05-27-funded-projects/",
    "title": "On-going Funded Projects 2021",
    "description": "QML members are active in externally funded projects. Here is a list of on-going funded projects by QML members as of May 2021.",
    "author": [
      {
        "name": "Akihito Kamata",
        "url": {}
      }
    ],
    "date": "2021-05-27",
    "categories": [],
    "contents": "\nKamata, A., Potgieter, C., Nese, J. F., & Kara, Y. Developing Computational Tools for Model-based Oral Reading Fluency Assessments. Funded by Institute of Educational Science - U.S. Department of Education to Southern Methodist University (August 2020 – July 2023). Total award $899,901.\nNese, J. F., Kamata, A., Saez, L., & Nese, R. A Comprehensive Measure of Reading Fluency: Uniting and Scaling Accuracy, Rate, and Prosody. Funded by Institute of Educational Science - U.S. Department of Education to University of Oregon (July 2020 – June 2024). Total award $1,399,379.\nBaker, D., Kamata, A., Larson, E., Richards-Tutor, C. Measuring the English Language Vocabulary Acquisition of Latinx Bilingual Students (Project MELVA-S). Funded by Institute of Educational Science - U.S. Department of Education to Southern Methodist University (August 2020 – July 2024). Total award $1,399,943.\n\n\n\n",
    "preview": {},
    "last_modified": "2022-02-11T14:53:03-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-02-28-pubs-03-2020/",
    "title": "Publications in 2020",
    "description": "QML members are active publishing papers in peer-reviewed journals. Here is a list of publications by QML members in 2020.",
    "author": [
      {
        "name": "Akihito Kamata",
        "url": {}
      }
    ],
    "date": "2020-05-08",
    "categories": [],
    "contents": "\nHsieh, M., & Kamata, A. (in press). Examining the pre-service school principals’ impromptu speech skill with a many-facet Rasch model. Journal of Applied Measurement.\nKara, Y., Kamata, A., Potgieter, C., & Nese, J. F. T. (2020). Estimating model-based oral reading fluency: A Bayesian approach. Educational and Psychological Measurement. doi:10.1177/0013164419900208\nLiang, X. (2020). Prior sensitivity in Bayesian structural equation modeling for sparse factor loading structures. Educational and Psychological Measurement. doi:10.1177/2F0013164420909885\nLiang, X., Kamata, A., & Li, J. (2020). Hierarchical Bayese approach to estimate the treatment effect for randomized controlled trials. Educational and Psychological Measurement. doi:10.1177/0013164420909885\nLiang, X., Yang, Y., & Cao, C. (2020). The performance of ESEM and BSEM in structural equation models with ordinal indicators. Structural Equation Modeling: A Multidisciplinary Journal.\n\n\n",
    "preview": {},
    "last_modified": "2022-02-11T14:53:03-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-02-28-aerancme-2020/",
    "title": "AERA/NCME 2020",
    "description": "QML members will be active at 2020 AERA/NCME Annual Meeting in San Francisco: April 16 - 21, 2020. Here is the list of presentations by QML members.  \nUPDATE: Unfortunately, the conference has been canceled due to concerns related COVID-19.",
    "author": [
      {
        "name": "Akihito Kamata",
        "url": {}
      }
    ],
    "date": "2020-02-28",
    "categories": [],
    "contents": "\nEquating Oral Reading Fluency Scores: A Model-Based Approach\n- Kamata, A., Kara, Y., Potgieter, C., & Nese, J. F. T.\n- This study demonstrates and evaluates equating procedures for the oral reading fluency (ORF) scores estimated by a latent binomial-lognormal joint model. Preliminary results showed that model-based ORF scores should be preferred over observed words correct per minute measures for passages that have been equated by the common-item non-equivalent group design.\nTwo Parameter Multilevel IRT Model with Heterogeneous Within-Group Variances\n- Kara, Y., & Kamata, A.\n- This study proposes a two-parameter multilevel IRT model with heterogeneous within-group variances. We introduce the model equations along with parameter estimation procedure adopting the Bayesian approach.\nPrediction of Reading Fluency Scores with Silence Tendencies Using Machine Learning Algorithms\n- Patarapichayatham, C., Kamata, A., Kara, Y., & Le, N. T.\n- This study investigates evidence for meaningful pauses that are believed to be made by fluent readers in the context of oral reading fluency (ORF) assessments. We derive various variables from between-word silence relative to total reading time and fit machine learning algorithms to classify readers into groups of ORF levels.\nPreliminary Consequential Validity Evidence for a Computerized Oral Reading Fluency Assessment\n- Nese, J. F. T., Anderson, D. J., & Kamata, A.\n- Curriculum-based measurement of oral reading fluency (ORF) is used to identify students at-risk for poor learning outcomes through screening assessments, and to monitor student progress to help guide and inform instructional decision-making. The purpose of this study was to compare the consequential validity properties of CORE and a traditional ORF assessment (easyCBM) for students in Grades 2 through 4. We found good evidence for the predictive and concurrent validity of CORE (comparable to traditional ORF), and improved properties of reliability for CORE compared to traditional ORF from a longitudinal design. We discuss the implications of these results for practitioners using these classroom assessments.\nExamining the Preservice School Principals’ Impromptu Speech Skills With a Many-Facet Rasch Model\n- Hsieh, M. C., & Kamata, A.\n- The Purpose of this study is to investigate the usefulness of the Many-Facet Rasch Model (MFRM) in evaluating the impromptu speech skills of pre-service principals. Different Facets, including the ability of pre-service principals, scoring criteria, rater severity, rater gender, and topic types are examined. Data with a sample of 186 pre-service principals rated by 9 raters were used in this study. The results of this study show that the MFRM provides a scientific approach to assessment which can reveal some diagnostic information from the original ordinal rating scores.\nSpanish and English Biliteracy: A Linear Growth Model for Two Parallel Processes\n- Patarapichayatham, C., & Locke, V. N.\n- This study investigated how growth in English and Spanish language literacy skills were related in a biliteracy setting. Data were collected in a school district in New Mexico in first, second, and third grades in 2018-19 school year. The classrooms were a two-way dual language immersion model that begins in kindergarten with instruction that is 80% in Spanish. At each subsequent grade level, the proportion is altered until it reaches 50/50 by third grade. Each student took ISIP English and ISIP Spanish assessments monthly from October to May. A linear growth model for two parallel processes under SEM framework is utilized using Mplus software. Results showed that a student develops their biliteracy skills faster in first grade.\nEarly Identification of Dyslexia: Can the Istation Indicators of Progress (ISIP) Screen for Risk in Kindergarten?\n- Locke, V. N. & Patarapichayatham, C.\n- The ISIP-ER is a formative assessment used by over 4 million school children in the United States. It is used as both a benchmarking as well as progress monitoring assessment. This research investigates if the ISIP-ER can assess the risk of dyslexia as early as fall of kindergarten. Significant differences between students at risk for dyslexia and those who are not are found in the Overall ISIP score, Phonemic Awareness, and Letter Knowledge. Use of the Istation Integrated Learning System may mediate the risk of dyslexia.\n\n\n",
    "preview": {},
    "last_modified": "2022-02-11T14:53:03-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-04-14-aerancme-2019/",
    "title": "AERA/NCME 2019",
    "description": "QML members were active at 2019 AERA/NCME Annual Meeting in Toronto, Canada: April 4 - 9, 2019. Here is the list of presentations by QML members.",
    "author": [
      {
        "name": "Akihito Kamata",
        "url": {}
      }
    ],
    "date": "2019-04-10",
    "categories": [],
    "contents": "\nEvaluation of Covariate Balance as a Quality Indicator for Propensity Score Matching Analysis\nKamata, A., Gallegos, E., Patarapichayatham, C., & Kara, Y.\nPropensity score analysis is becoming increasingly popular in educational research. Examining the covariate balance is considered to be crucial to justify the quality of propensity score analysis results. However, it has been pointed out that solely considering how covariates balance after matching may not be enough for justifying the quality of the propensity score analysis result. Suitable covariate balance may still yield biased estimates of treatment effects. The current study aims to systematically demonstrate this problem by a series of simulation studies by investigating the effect of the number of covariates, types of covariates, and degrees of association between covariates and treatment and/or outcome in the propensity score model.\n\n\nHierarchical Bayesian Approach to Estimate Interventional Effect in Randomized Control Trials\nLiang, X., Kamata, A., Sirganci, G., & Li, J.\nWe propose a framework to construct hyper priors for both the mean and variance hyperparameters for estimating interventional effects in a two-group randomized control trial. One important issue in Bayesian estimation is the determination of an effective informative prior, especially with small sample sizes. With hierarchical Bayesian modeling, the uncertainty of hyperparameters of a prior can be further modeled via their own priors, namely, hyper priors. The performance of hierarchical Bayesian models was compared with empirical Bayesian models where hyperparameters were constants. In a preliminary analysis, the hierarchical Bayesian approach showed promising improvement on posterior inferences compared to the empirical Bayesian approach.\n\n\nEvaluation of the Utility of Informative Priors in SEM with Small Samples\nMa, H., Kamata, A., & Kara, Y.\nThis study evaluates the performance of different estimators on factor loadings and structural coefficients in terms of bias, RMSE, and SE for factor analysis and SEM models under the ML and Bayesian framework with small sample settings. Simulation conditions varied in sample sizes, mean factor loading, priors, and estimators.\n\n\nPower Analysis for Moderated Mediation Models with Continuous Moderator Variable\nKamata, A., Kara, Y., Liang, X., & Le, N.\nAlthough there have been a number of studies in the literature that made contributions to guide practitioners on performing power analyses for mediation models, not many focused on moderated mediation models. Although some demonstrated power analysis with moderated mediation effect, they were only with categorical moderator variables with very few categories. The aim in this study is to fill the mentioned gaps in the literature by demonstrating the power analysis of moderated mediation models with continuous moderator variables so that practitioners can implement such analysis.\n\n\nQuantifying Reliability for Oral Reading Fluency Assessment using the Grubbs Model\nPotgieter, C., & Kamata, A.\nThis study proposes to apply the Grubbs model to estimate oral reading fluency more accurately. It is demonstrated that application of the Grubbs model allows quantifying measurement error variance to determine optimal weights to combine passage-level data to improve the fluency score for each student.\n\n\nClassification Predictors for Reading Fluency in Oral Reading Fluency Assessment\nKamata, A., Kara, Y., Patarapichayatham, C., Le, N.\nThis study investigates potential predictors of reading fluency in student oral reading fluency assessment passages. We derive a number of variables from word-level reading time, silence time, and accuracy. Then, we fit machine learning algorithms to explore how the derived variables classify readers into groups of different levels of fluency.\n\n\nDIF Analysis for Immigrant Status for the 2015 PISA Science Items\nUsta, G., & Kamata, A.\nThe purpose of this study is to use Differential Item Functioning (DIF) analysis to investigate differences in the performance of immigrant and non-immigrant students in 48 cognitive science items for the U.S. sample in 2015 PISA. According to the results, all items displayed level-A DIF, indicating that the items had negligible DIF effect.\n\n\nThe Use of Automated Scoring to Evaluate the In-Depth Vocabulary Knowledge of English Learners\nBaker, D. L., Sano, M., Le, N., Collazo, M., & Kamata, A.\nThe purpose of this paper is to (a) test a new automated system developed to score the hand transcribed speech data of second grade Hispanic students, and (b) compare the accuracy of two machine learning techniques. Speech from 217 Hispanic English Learners who were part of a larger study were analyzed using support vector machine (SVM) and tree-based regression (TBR). Findings indicate that the reliability of the automated scoring systems were comparable to human scoring, and that when comparing SVM and TBR, the latter appeared to improve higher Quadratic Weighted Kappa than SVM. Implications of this study are discussed in the context of finding new ways to analyze student natural speech.\n\n\nSchool Leader Instructional Support and Change in Novice Teachers’ Efficacy\nWilhelm, A. G., Woods, D. M. & Yusuf Kara, Y.\nNovice teachers’ self-efficacy provides a lens through which we might understand how their self-perceptions of competence relate to teaching practices and retention. A number of cross-sectional studies have reported relations between school leader support, especially instructional leadership, and teacher self-efficacy. In this study we examine change in novice teachers’ self-efficacy using growth models within the structural-equational modeling framework and relate change in self-efficacy to change in teachers’ perceptions of school leader instructional support over time. Our results add support to the notion that school leaders’ instructional leadership is related to teachers’ self-efficacy and that instructional leadership can actually support growth in self-efficacy, at least in the case of first-year teachers.\n\n\nImpacts of Measurement Error in Pretest on Treatment Effect Estimate in Pretest-and-Post Design Analysis\nMiyazaki, Y., Kamata, A., & Uekawa, K.\nIn program evaluation, evaluating the treatment effect accurately is of most importance. In order to achieve the goal, random assignment is desirable, but in reality the randomization can be imperfect, in which case preexisting difference will exist between treatment conditions. Furthermore, there always will be measurement errors in pretest. These two factors can create serious challenges for evaluators to accurately gauge the treatment effect and the impacts of these factors are not well known. Thus, in this paper, we addressed how much bias occurs in estimating treatment effect when the random assignment is imperfect and when there is a measurement error in pretest in evaluation studies that use the pretest-and-posttest design.\n\n\nEvaluation of the Education for Sustainability in Galapagos Program\nRahim, H., Luna, H., & Kamata, A.\nAny program that seeks to improve its program as well as prove its outcomes benefits from a holistic evaluation approach. As the primary research and evaluation arm of the Education for Sustainability in Galapagos project, we ask and answer questions regarding how the program is going and trying to understand what changes, if any, occur in educators’ instructional practice and attitudes in specific constructs (e.g., grit, growth mindset, peer collaboration). In the process of doing so, we also examine fidelity of implementation and dosage of intervention to identify the key factors of the implementation that might lead to creating these potential outcome changes. In order to address needs of myriad stakeholders and implementers, the evaluation of this program utilizes a blend of developmental, empowerment, formative, process, and outcome evaluation approaches for a comprehensive evaluation that caters to different needs and for different purposes. For the purposes of this presentation, the focus will be on formative, process, and outcome data.\n\n\n",
    "preview": {},
    "last_modified": "2022-02-11T14:53:03-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-04-15-tuesap-2019/",
    "title": "TUESAP 2019",
    "description": "QML organized Texas Universities Educational Statistics and Psychometrics (TUESAP) conference (Feb 28 & Mar 1, 2019). [TUESAP website](https://www.smu.edu/simmons/Research/CORE/TUESAP)",
    "author": [
      {
        "name": "Akihito Kamata",
        "url": {}
      }
    ],
    "date": "2019-03-01",
    "categories": [],
    "contents": "\n\n\n\n",
    "preview": {},
    "last_modified": "2022-02-11T14:53:03-06:00",
    "input_file": {}
  },
  {
    "path": "posts/welcome/",
    "title": "Welcome to QML Website",
    "description": "Quantitative Methods Lab is a research group at Southern Methodist University. We are not officially affiliated to any specific academic/service unit, but we are housed at the Center on Research and Evaluation (CORE) at SMU. We actively study and conduct research on various topics on quantitative methods in behavioral science and educational research.",
    "author": [
      {
        "name": "Akihito Kamata",
        "url": {}
      }
    ],
    "date": "2019-01-01",
    "categories": [],
    "contents": "\n\n\n\n",
    "preview": {},
    "last_modified": "2022-02-11T14:53:03-06:00",
    "input_file": {}
  }
]
